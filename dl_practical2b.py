# -*- coding: utf-8 -*-
"""Dl_Practical2B.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uI2PtgMYRqIxFW37kfvI8-xiisbimmVH
"""

import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

vocab_size=1000
(x_train,y_train),(x_test,y_test) = imdb.load_data(num_words=vocab_size)

tokenizer= Tokenizer(num_words=vocab_size)
X_train = tokenizer.sequences_to_matrix(x_train,mode='binary')
X_test = tokenizer.sequences_to_matrix(x_test,mode='binary')

max_length=500
x_train=pad_sequences(X_train,maxlen=max_length,padding='pre',truncating='pre')
x_test=pad_sequences(X_test,maxlen=max_length,padding='pre',truncating='pre')

model= tf.keras.models.Sequential([
    tf.keras.layers.Embedding(vocab_size,64,input_length=max_length),
    tf.keras.layers.Conv1D(128,5,activation='relu'),
    tf.keras.layers.GlobalMaxPooling1D(),
    tf.keras.layers.Dense(64,activation='relu'),
    tf.keras.layers.Dense(1,activation='sigmoid')
])

model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

history=model.fit(x_train,y_train,epochs=10,batch_size=32,validation_split=0.2)

test_loss,test_acc=model.evaluate(x_test,y_text)
print("test accuraccy : ",test_acc)

new_x=["the movie was excellent and worth watching"]
new_x=tokenizer.texts_to_sequences(new_x)
new_x=pad_sequences(new_x,maxlen=max_length,padding='pre',truncating='pre')
pred=model.predict(new_x)[0][0]
if pred>0.5:
  print("Positive Review")
else:
  print("Negative Review")

